---
layout: post
title: '[Paper Review] A Simple Framework for Contrastive Learning of Visual Representations'
subtitle: SimCLR
date: '2022-02-03'
categories:
    - study
tags:
    - self-supervised-learning, simclr
related_posts:
  - _posts/study/2022-06-09-deep_embedded_clustering.md
comments: true
published: true
last_modified_at: '2022-02-03'
---

Contrastive learning 기반의 self-supervised learning을 활용한 SimCLR를 알아보고자 합니다. 설명에 앞서 님의 블로그를 참고하였음을 밝힙니다.

- Table of Contents
{:toc .large-only}


## Self-supervised learning

***

> Self-supervised learning은 label 없이 Input 그 자체만을 활용하여 학습하는 방법을 의미합니다. 


## Contrastive learning

***

> Contrastive learning은 positive pair끼리는 같에, negative pair 끼리는 다르게 구분하면서 모델을 학습하는 벙법입니다. Input의 label 정보를 사용하지 않는 unsupervised learning으로 데이터의 일반적인 특징(general features)을 학습하기 위해 사용됩니다.



## 1. Introduction

***

* Human supervision(인위적으로 부여된 label) 없이 visual representation을 학습하는 것은 오래동안 풀리지 않은 문제였다고 합니다.
* Self-supervised learning의 방법은 크게 generative 또는 discriminative apporach로 나뉘었습니다.
    - Generative: 주어진 train data를 학습하여 train data의 분포를 따르는 유사한 데이터를 생성하는 방법이지만, 픽셀단위의 generation은 계산량이 많고 이렇게 해서 얻어진 결과가 representation learning에 적합하지 않을 수 있다고 합니다.
    - Discriminative: 이미지를 잘라 zigsaw 퍼즐을 만든 후 모델이 퍼즐을 풀수 있게끔 학습하는 방법과 같이, pretext task를 주어서 모델이 학습하게 하고 그 과정에서 representation을 학습하는 것입니다. 그러나 이 방법은 heuristic에 많은 부분을 의존하고 학습된 representation의 generality를 보장할 수 없다는 한계가 있습니다.

본 논문에서는 constrasive learnig에 기반한 discriminative approaches를 통해 SOTA를 달성하였습니다.
{:.lead}


## 2. Method

***

SimCLR은 하나의 데이터에서 증강된 두 개의 데이터 간의 agreement를 최대화함으로써 representation을 학습합니다.

![Simple framework](https://github.com/HayoonSong/Images-for-Github-Pages/blob/main/study/paper_review/2022-02-03-SimCLR/Framework.PNG?raw=true){: width="50%" height="50%"}{:.aligncenter}
### 2.1 The Contrastive Learning Framework
<br/>

* **Data augmentation module ($t$~$T$)**
    - 이미지 $x$를 변환하여 $x_i$ 및 $x_j$를 생성하는 데이터 증강 모듈입니다.
    - 3개의 augmentation인 random cropping, random color distortions (jitter or drop), random Gaussian blur를 순차적으로 적용하였습니다.    
    ![Data augmentation final](https://github.com/HayoonSong/Images-for-Github-Pages/blob/main/study/paper_review/2022-02-03-SimCLR/data_augmentation_final.jpg?raw=true){:.aligncenter}
    <br/>
    <br/>

* **Base encoder ($f$)**
    - 생성된 2개의 이미지($x_i$ 및 $x_j$)에서 repersentation vectors를 추출하는 인코더입니다.
    - Base encoder로 이미지 분류에서 일반적으로 사용되는 ResNet-50을 활용하였습니다.
    - $h_i = f(x_i) = ResNet(x_i)$
    - $h_i$는 Global average pooling을 거쳐서 vectors로 나옵니다.
    <br/>
    <br/>

* **Projection head($g$)**
    - Representations을 contrastive loss의 latent space로 mapping해주는 header입니다.
    - 1개의 hidden layer를 갖고 있는 MLP를 사용했으며 중간에 Relu 함수를 적용하였습니다.
    - $ z_i = g(h_i) = W^{(2)}σ(W^{(1)}h_i) $
    <br/>
    <br/>

* **Contrastive loss function**
    - 하나의 이미지에서 증강된 $x_i$와 $x_j$는 positive pair 입니다.
    - $x_i$ 및 $x_j$를 포함하는 세트 {$x_k$}가 주어지면 contrastive task는 {$x_k$}$_{k≠i}$에서 $x_i$의 positive pair인$x_j$를 식별하는 것을 목표로 합니다.
    - $l_2$ normalize
    <br/>
    <br/>

미니배치 N에서 한 이미지당 2번의 증강을 거쳐 총 2N 개의 데이터가 생성됩니다.이 때 한 이미지에서 증강된 2개의 이미지는 positive pair가 됩니다. Negative examples을 따로 샘플링하지 않았습니다. 대신 positive pair가 주어지면 미니배치 내의 2(N-1)개의 증강된 데이터를 positive examples로 취급하였습니다. $ sim(υ, ν) = υ^Tν/\Vert υ \Vert \Vert ν \Vert $ 는 $ l_2 $ normalized된 $ υ $ 와 $ ν $ 사이의 내적(즉, 코사인 유사도)을 나타냅니다. Positive pair($ i, j$)에 대한 손실 함수는 다음과 같이 정의됩니다.


$$
l_{i,j} = - \log \frac{\exp(sim(z_i,z_j)/τ)}{\sum_{k=1}^{2N}1_{[k≠i]}\exp(sim(z_i,z_k)/τ)}
$$

$1_{[k≠i]} ∈ \{0, 1\}$는 $k≠i$일 때 $1$로 평가되는 indicator function이며, $τ$는 temperature parameter입니다. 최종 손실은 미니배치서 ($i,j$) 및 ($j,i$)의 모든 positive pairs에 대해 계산됩니다. 이를 NT-Xent (the normalized temperature-scaled cross entropy loss)라고 합니다.

### 2.2 Training with Large Batch Size
<br/>

Batch size는 256에서 8192까지 다양하게 설정하였으며 batch size가 8192일 때는 한 쌍의 positive pair에 대해 16382개의 negative examples를 활용할 수 있었습니다. 큰 batch size와 SGD/Momentum을 사용할 경우에 학습이 불안정해질 수 있으므로 LARS optimizer를 사용하였습니다. 또한 locally하게 Batch normalization을 적용하지 않고 전체 데이터셋에 대한 Global Batch Normalization을 적용하였습니다.

### 2.3 Evaluation Protocol   
<br/>

* Dataset
    - ImageNet ILSVRC-2012 dataset
    - CIFAR-10 (Appendix)
    <br/>
    <br/>      
* Metrics
    - Linear evaluation
    : 학습된 모델을 고정(freeze)하고 맨 끝에 linear classifier를 추가하여 성능을 평가하는 방법입니다.
    - Semi-supervised learning
    - Trnasfer learning
    <br/>
    <br/> 
* Default setting
    - Learning rate: 4.8 (= 0.3 * BatchSize/256)
    - Weight ecay: 10^-6
    - Batch size: 4096
    - Epochs: 100
    - Linear warm up for the first 10 epoochs
    - Decay the learning rate with the cosine decay schedule without restarts


## 3. Data Augmentation for Contrastive Representation Learning

***
## 3-1. Composition of data augmentation operations is crucial for learning good representations

데이터 증강의 유형 중에서 가장 좋은 성능을 보이는 방법을 찾기 위한 실험을 진행하였습니다. 개별적인 데이터 증강과 증강 구성의 중요성을 조사하였습니다.

* 공간적/기하학적 변형 (spatial/geometric transformation)
    - Cropping and resizing (with horizontal flipping)
    - Rotation
    - Cutout 

* 모양 변형 (appearance transformation)
    - Color distortion (including color dropping, brightness, contrast, saturation, hue)
    - Gaussian blur
    - Sobel filtering

![Data augmentation](https://github.com/HayoonSong/Images-for-Github-Pages/blob/main/study/paper_review/2022-02-03-SimCLR/data_augmentation.png?raw=true){:.aligncenter}

ImgaeNet 이미지는 크기가 다르기 때문에 데이터 증강 이전에 기본적으로 이미지 자르기 및 크기 조정을 적용하였습니다.  

![Data augmentation confusion matrix](https://github.com/HayoonSong/Images-for-Github-Pages/blob/main/study/paper_review/2022-02-03-SimCLR/data_augmentation_cm.PNG?raw=true){:.aligncenter}

## 4. Architectures for Encoder and Head

***

### 4.1 Unspervised contrastive learning benefits (more) from bigger models


### 4.2 A nonlinear projection head improves the representation quality of the layer before it

<br>

3 가지의 다른 헤드 아키텍처를 사용한 선형 평가(linear evaluation) 결과는 다음과 같습니다.

* Linear: 여러개의 이전의 여러 접근 방식에서 사용된 선형 투영  
* Non-linear: 하나의 추가 은닉층(및 ReLU 활성화)이 있는 기본 비선형 투영   
* None: identity mapping  

![Projection head](https://github.com/HayoonSong/Images-for-Github-Pages/blob/main/study/paper_review/2022-02-03-SimCLR/projection_head.PNG?raw=true){:.aligncenter}

그림 5는 하나의 branch에만 적용되는 개별 데이터 변환 기법 또는 구성 데이터 확장에 따른 선형 평가(ImageNet top-1 정확도)를 나타냅니다. 마지막 열을 제외한 모든 열에 대해 대각선 항목은 단일 변환에 해당하고 비대각선 항목은 두 변환의 구성에 해당합니다(순차적으로 적용됨). 마지막 열은 행의 평균을 반영합니다.

개별적인 데이터 변환의 영향과 증강 구성의 중요성을 이해하기 위해서, 저자들은 데이터 증강 기법을 개별적으로 적용했을 때와 쌍을 이우어서 적용했을 때를 비교하였습니다.

ImageNet의 이미지 크기가 서로 상이하였기에 `crop` 과 `resize`는 항상 적용하였습니다. 이 방법은 cropping이 부재한 다른 증강 기업을 연구하는 것에 어려움을 주었습니다.(?) 


* **Non-linear** vs Linear   
비선형 투영은 선형 투영보다 약 3% 더 좋은 성능을 보였습니다.   
* **Non-linear** vs None   
비선형 투영은 투영이 없는 것보다 약 10% 이상 훨씬 좋은 성능을 보였습니다.    

투영 헤드를 사용할 경우 출력 차원(output dimension)과 관계없이 유사한 결과가 관찰되었습니다. 또한 비선형 투영을 사용하더라도 투사 헤드 이전의 레이어 h(Base encoder)는 투사 헤드 이후의 레이어 z = g(h)보다 훨씬 더 우수하며, 이는 투사 헤드 이전의 은닉 레이어가 투사 헤드 이후의 레이어보다 더 나은 표현임을 보여줍니다.

## 6. Comparison with State-of-the-art

***

* **Linear evaluation**

## Appendix

***

### A. Data Augmentation Details

* Random crop and resize to 224 x 224
* Color distortion
* Gaussian blur

### B. Additional Experimental Results

#### B.1. Batch Size and Training Steps

#### B.5. Semi-supervised Learning via Fine-Tuning


#### B.6. Linear Evaluation

#### B.7. Correlation Between Linear Evaluation and Fine-Tuning

다양한 trainin step 및 네트워크 아키텍처 설정에서 linear evalutation과 fine-tuning의 상관관계를 연구하였습니다.

![Coreelation Between Linear Evaluation and Fine-Tuning](https://github.com/HayoonSong/Images-for-Github-Pages/blob/main/study/paper_review/2022-02-03-SimCLR/correlation_le_ft.PNG?raw=true){:.aligncenter}

ResNet-50(배치 크기 4096 사용)의 훈련 에포크가 50에서 3200까지 다양할 때 선형 평가 대 미세 조정을 보여줍니다. 거의 선형적으로 상관 관계가 있지만, 레이블의 작은 부분을 미세 조정하는 것이 더 긴 훈련에서 더 많은 이점을 얻는 것 같습니다.



## References
[1] Predictive Intelligence in Medicine: 4th International Workshop, PRIME 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, October 1, 2021,

<br>
<br>

***

<center>오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다.</center>