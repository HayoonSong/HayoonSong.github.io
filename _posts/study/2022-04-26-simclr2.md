---
layout: post
title: '[Paper Review] Big Self-Supervised Models are Strong Semi-Supervised Learners'
subtitle: SimCLR2
date: '2022-04-26'
categories:
    - study
tags:
    - simclr2
comments: true
published: true

last_modified_at: '2022-04-26'
---

- Table of Contents
{:toc .large-only}

## 0. Abstract

***

- Key ingredient: use of big (deep and wide) networks during pretraining and fine-tuning

- The proposed semi-supervised learning alogrithm
    1. Unsupervised pretraining of a big ResNet model using SimCLRv2
    2. supervised fine-tuning on a few labeled examples
    3. Distillation with unlabeled examples for refining and transferring the task-speicifc knowledge.

- 1%의 labels(한 class 당 13개 이하의 labeled images)만을 가지고 ResNet-50을 활용하여 ImgeNet top-1 accuracy 73.9%를 달성하였습니다.
- 10%의 labels을 가지고 ResNet-50을 사용하여 77.5%를 달성하였습니다. 

## 1. Introduction

***
대량의 unlabeled data를 최대한 활용하면서 소량의 labeled examples로 학습하는 것은 머신러닝에서 오랜 문제입니다.
- semi-supervised learning:
Semi-supervised learning은 일반적으로 pretraining 및 finetuning으로 이루어집니다.
Pretraining에서는 unsupervised learing 또는 self-superviesd learning으로 데이터 자체의 본직적인 특성을 학습하며,
finetuning에서는 labeled data를 활용하여 supervised learning을 함으로써 약간의 가이드로 일반화 성능을 끌어올리는 것을 목표로 합니다.



## References
[1] semi-supervised learing
https://sanghyu.tistory.com/177#:~:text=Semi%2Dsupervised%20learning%20(%EC%A4%80%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5)%EC%9D%80%20%EC%86%8C%EB%9F%89%EC%9D%98,%EB%AA%A9%ED%91%9C%EB%A1%9C%20%ED%95%98%EB%8A%94%20%EB%B0%A9%EB%B2%95%EB%A1%A0%EC%9D%B4%EB%8B%A4.

